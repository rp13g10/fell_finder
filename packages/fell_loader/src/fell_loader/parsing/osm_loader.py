"""Functions relating to the loading of a .osm.pbf network graph into a tabular
format"""

import json
import os
from glob import glob

# import daft
from pyspark.sql import DataFrame, functions as F, SparkSession, Window
from pyspark.sql import types as T
from bng_latlon import WGS84toOSGB36


# TODO: Set this up to work with other filenames once moving to a fully
#       automated pipeline

# TODO: Plan out logic for using output from osm-parquetizer w. Daft. Need to
#       unpack the 'ways' output into edges. Expected schemas are:

# Nodes
# |-- id: long (nullable = true)
# |-- version: integer (nullable = true)
# |-- timestamp: long (nullable = true)
# |-- changeset: long (nullable = true)
# |-- uid: integer (nullable = true)
# |-- user_sid: string (nullable = true)
# |-- tags: array (nullable = true)
# |    |-- element: struct (containsNull = true)
# |    |    |-- key: string (nullable = true)
# |    |    |-- value: string (nullable = true)
# |-- latitude: double (nullable = true)
# |-- longitude: double (nullable = true)

# Ways
# |-- id: long (nullable = true)
# |-- version: integer (nullable = true)
# |-- timestamp: long (nullable = true)
# |-- changeset: long (nullable = true)
# |-- uid: integer (nullable = true)
# |-- user_sid: string (nullable = true)
# |-- tags: array (nullable = true)
# |    |-- element: struct (containsNull = true)
# |    |    |-- key: string (nullable = true)
# |    |    |-- value: string (nullable = true)
# |-- nodes: array (nullable = true)
# |    |-- element: struct (containsNull = true)
# |    |    |-- index: integer (nullable = true)
# |    |    |-- nodeId: long (nullable = true)


# Nodes
# - Select id, latitude and longitude
# - Check in tags for anything useful

# Edges
# - Start with Ways
# - Explode out to one record per way ID, per node, retaining index in way
# - Get dst with offset
# - Extract oneway flag from tags
# - Generate reverse edges where appropriate
# - Join on nodes to get lat/lon


class OsmLoader:
    """Reads in the contents of the provided OSM extract. Generates two parquet
    datasets containing details of the graph nodes and edges. These datasets
    are partitioned according to the british national grid system to improve
    access speed in downstream applications."""

    def __init__(self, spark: SparkSession) -> None:
        """Create an instance of the OSM loader, store down the directory which
        contains the data to be loaded."""

        self.spark = spark

        self.data_dir = os.environ["FF_DATA_DIR"]
        self.binary_loc = os.environ["FF_PARQUETIZER_LOC"]

        self.nodes_loc = ""
        self.ways_loc = ""

    def _set_parquet_locs(self) -> None:
        """Store the location of the parquet files generated by osm-parquetizer
        as a class attribute for convenience.

        Raises:
            FileNotFoundError: If no .parquet files are detected, an exception
                will be raised.

        """
        osm_loc = os.path.join(self.data_dir, "extracts", "osm")

        nodes_loc = glob(os.path.join(osm_loc, "*.node.parquet"))
        ways_loc = glob(os.path.join(osm_loc, "*.way.parquet"))

        try:
            self.nodes_loc = nodes_loc[0]
            self.ways_loc = ways_loc[0]
        except IndexError:
            raise FileNotFoundError(
                f"Unable to find generated parquet files in {osm_loc}. Please "
                "ensure you have called unpack_osm_pbf, and check the job"
                "logs"
            )

    def _unpack_osm_pbf(self) -> None:
        """Use the osm-parquetizer utility to unpack the provided osm.pbf file
        in the extracts/osm directory into parquet files. These can then be
        read in as dataframes for further processing.

        Raises:
            FileNotFoundError: If no osm.pbf file is found in the extracts/osm
                subfolder of the data directory, an exception will be raised
            RuntimeError: If multiple osm.pbf files are found in the
                extracts/osm subfolder of the data directory, an exception will
                be raised

        """

        pbf_loc = glob(
            os.path.join(self.data_dir, "extracts", "osm", "*.osm.pbf")
        )

        if not pbf_loc:
            raise FileNotFoundError(f"No osm.pbf file detected in {pbf_loc}")
        elif len(pbf_loc) > 1:
            pbf_str = ", ".join(pbf_loc)
            raise RuntimeError(
                f"Multiple osm.pbf files detected in {pbf_loc}: {pbf_str}"
            )
        else:
            pbf_loc = pbf_loc[0]

        os.system(f"java -jar {self.binary_loc} {pbf_loc}")

        self._set_parquet_locs()

    def read_osm_data(self) -> tuple[DataFrame, DataFrame]:
        """Read in the contents of the OSM nodes/ways parquet files as
        daft dataframes. Note that this can only be called after
        unpack_osm_pbf.

        Returns:
            A dataframe containing the nodes, and a dataframe containing the
            ways

        """

        try:
            self._set_parquet_locs()
        except FileNotFoundError:
            self._unpack_osm_pbf()
            self._set_parquet_locs()

        nodes_df = self.spark.read.parquet(self.nodes_loc)
        ways_df = self.spark.read.parquet(self.ways_loc)

        return nodes_df, ways_df

    @staticmethod
    def assign_bng_coords(nodes: DataFrame) -> DataFrame:
        """Assign each node an easting and northing, this will be used to
        split the nodes dataset into partitions according to their geographical
        location

        Args:
            nodes: The nodes dataframe

        Returns:
            A copy of the input dataframe with additional easting and northing
            columns

        """

        @F.udf(
            returnType=T.StructType(
                [
                    T.StructField("easting", T.DoubleType()),
                    T.StructField("northing", T.DoubleType()),
                ]
            )
        )
        def bng_udf(lat: float, lon: float) -> dict[str, float]:
            easting, northing = WGS84toOSGB36(lat, lon)
            return dict(easting=easting, northing=northing)

        nodes = nodes.withColumn(
            "bng_coords", bng_udf(F.col("latitude"), F.col("longitude"))
        )

        nodes = nodes.withColumn(
            "easting",
            F.col("bng_coords.easting").astype(T.IntegerType()),
        )

        nodes = nodes.withColumn(
            "northing",
            F.col("bng_coords.northing").astype(T.IntegerType()),
        )

        nodes = nodes.drop("bng_coords")

        return nodes

    # Nodes
    # |-- id: long (nullable = true)
    # |-- version: integer (nullable = true)
    # |-- timestamp: long (nullable = true)
    # |-- changeset: long (nullable = true)
    # |-- uid: integer (nullable = true)
    # |-- user_sid: string (nullable = true)
    # |-- tags: array (nullable = true)
    # |    |-- element: struct (containsNull = true)
    # |    |    |-- key: string (nullable = true)
    # |    |    |-- value: string (nullable = true)
    # |-- latitude: double (nullable = true)
    # |-- longitude: double (nullable = true)

    @staticmethod
    def set_node_output_schema(nodes: DataFrame) -> DataFrame:
        """Ensure the node output dataset contains only the required columns

        Args:
            nodes: A polars dataframe containing details of all nodes in the
              graph

        Returns:
            A subset of the provided dataframe

        """
        nodes = nodes.select(
            "id",
            F.col("latitude").alias("lat"),
            F.col("longitude").alias("lon"),
            "easting",
            "northing",
        )
        return nodes

    # Ways
    # |-- id: long (nullable = true)
    # |-- version: integer (nullable = true)
    # |-- timestamp: long (nullable = true)
    # |-- changeset: long (nullable = true)
    # |-- uid: integer (nullable = true)
    # |-- user_sid: string (nullable = true)
    # |-- tags: array (nullable = true)
    # |    |-- element: struct (containsNull = true)
    # |    |    |-- key: string (nullable = true)
    # |    |    |-- value: string (nullable = true)
    # |-- nodes: array (nullable = true)
    # |    |-- element: struct (containsNull = true)
    # |    |    |-- index: integer (nullable = true)
    # |    |    |-- nodeId: long (nullable = true)

    @staticmethod
    def unpack_tags(ways: DataFrame) -> DataFrame:
        """Transform the tags for each way from the input format:

            [{tag_1: value_1}, {tag_2: value_2}]

        into a more usable JSON struct:

            {tag_1: value_1, tag_2: value_2}

        Note that JSON must be used, as UDFs cannot currently return mappings.
        At present, there does not seem to be any built-in function which can
        achieve the desired effect.

        Args:
            ways: A dataframe containing the ways from the OSM extract

        Returns:
            A view of the input dataframe, with a processed tags column

        """

        # TODO: Confirm that utf8 is the correct encoding

        @F.udf(returnType=T.MapType(T.StringType(), T.StringType()))
        def tags_udf(tags: list[dict]) -> dict[str, str]:
            tags_dict = {}
            for tag in tags:
                key = tag["key"].decode("utf8")
                value = tag["value"].decode("utf8")
                tags_dict[key] = value

            return tags_dict

        ways = ways.withColumn("tags", tags_udf(F.col("tags")))

        return ways

    @staticmethod
    def get_tag_as_column(df: DataFrame, tag_name: str) -> DataFrame:
        df = df.withColumn(
            tag_name,
            F.regexp_replace(F.col("tags").getItem(tag_name), '"', ""),
        )

        return df

    def remove_restricted_routes(self, ways: DataFrame) -> DataFrame:
        """Remove any ways which correspond to routes with access controls,
        which are not usable by the public

        Args:
            ways: A dataframe containing the ways from the OSM extract

        Returns:
            A filtered copy of the input dataset

        """

        ways = self.get_tag_as_column(ways, "access")

        accessible_mask = F.col("access").isin(
            "yes", "permissive", "designated"
        )

        noflag_mask = F.col("access").isNull()

        ways = ways.filter(accessible_mask | noflag_mask).drop("access")

        return ways

    def set_flat_flag(self, ways: DataFrame) -> DataFrame:
        """For any edges which correspond to a feature which means data from
        a terrain model for elevation will not apply (i.e. a bridge or a
        tunnel), set a flag to ensure the elevation data for these edges is
        masked.

        Args:
            ways: A dataframe containing the ways from the OSM extract

        Returns:
            A copy of the input dataset with an additional boolean
            'is_flat' column

        """

        ways = self.get_tag_as_column(ways, "bridge")
        ways = self.get_tag_as_column(ways, "tunnel")

        ways = ways.withColumn(
            "is_flat",
            (F.col("bridge").isNotNull()) | (F.col("tunnel").isNotNull()),
        ).drop("bridge", "tunnel")

        return ways

    def set_oneway_flag(self, ways: DataFrame) -> DataFrame:
        """For any routes which are not one-way, we will need to create a
        reversed view to enable graph traversal in both directions. This
        information is recorded in the tags, so we must retrieve it for
        ease of use.

        Args:
            ways: A dataframe containing the ways from the OSM extract

        Returns:
            A copy of the input dataset with an additional boolean 'oneway'
            column

        """

        ways = self.get_tag_as_column(ways, "oneway")

        bidirectional_mask = F.col("oneway") == "no"
        noflag_mask = F.col("oneway").isNull()

        ways = ways.withColumn(
            "oneway",
            ~(bidirectional_mask | noflag_mask),
        )

        return ways

    # Ways
    # |-- id: long (nullable = true)
    # |-- version: integer (nullable = true)
    # |-- timestamp: long (nullable = true)
    # |-- changeset: long (nullable = true)
    # |-- uid: integer (nullable = true)
    # |-- user_sid: string (nullable = true)
    # |-- tags: array (nullable = true)
    # |    |-- element: struct (containsNull = true)
    # |    |    |-- key: string (nullable = true)
    # |    |    |-- value: string (nullable = true)
    # |-- nodes: array (nullable = true)
    # |    |-- element: struct (containsNull = true)
    # |    |    |-- index: integer (nullable = true)
    # |    |    |-- nodeId: long (nullable = true)

    @staticmethod
    def generate_edges(ways: DataFrame) -> DataFrame:
        # Bring through index as way_inx, as well as src node
        # Window fn to pull through dst
        # Drop any records with null dst
        edges = ways.select(
            F.col("id").alias("way_id"),
            F.col("tags"),
            F.col("is_flat"),
            F.col("oneway"),
            F.col("highway"),
            F.col("surface"),
            F.explode(F.col("nodes")).alias("node"),
        )

        edges = edges.withColumn("way_inx", F.col("node").getItem("index"))

        edges = edges.withColumn("src", F.col("node").getItem("nodeId"))

        window_spec = Window.partitionBy(F.col("way_id")).orderBy(
            F.col("way_inx")
        )

        edges = edges.withColumn("dst", F.lead(F.col("src")).over(window_spec))

        edges = edges.dropna(subset=["src", "dst"], how="any").drop("nodes")

        return edges

    @staticmethod
    def add_reverse_edges(edges: DataFrame) -> DataFrame:
        """For any edges A-B where the oneway field is either NULL or
        explicitlyset to no, create a second edge B-A. Assign this new edge
        to a new way, and record its position in the dataframe. The geometry
        of a way is set by the position of each edge in the dataframe, so it
        is important that this information be retained.

        Args:
            edges: A dataframe containing details of all edges in the graph

        Returns:
            A copy of the input dataframe, with any bi-directional edges
            explicitly represented in both directions

        """
        reverse = edges.filter(~F.col("oneway"))

        reverse = reverse.withColumn("old_src", F.col("src"))
        reverse = reverse.withColumn("src", F.col("dst"))
        reverse = reverse.withColumn("dst", F.col("old_src"))
        reverse = reverse.withColumn("way_id", F.col("way_id") * -1)
        reverse = reverse.withColumn("way_inx", F.col("way_inx") * -1)

        reverse = reverse.drop("old_src")

        out = edges.unionByName(reverse)

        return out

    @staticmethod
    def get_edge_start_coords(nodes: DataFrame, edges: DataFrame) -> DataFrame:
        """For each edge in the graph, work out its starting position by
        joining on details of the source node.

        Args:
            nodes: A polars dataframe containing details of all nodes in the
              graph
            edges: A polars dataframe containing details of all edges in the
              graph

        Returns:
            A copy of the edges dataframe with additional src_lat, src_lon,
            src_easting, src_northing columns

        """
        nodes = nodes.select(
            F.col("id").alias("src"),
            F.col("lat").alias("src_lat"),
            F.col("lon").alias("src_lon"),
            F.col("easting").alias("src_easting"),
            F.col("northing").alias("src_northing"),
        )

        edges = edges.join(nodes, on="src", how="inner")

        return edges

    @staticmethod
    def get_edge_end_coords(nodes: DataFrame, edges: DataFrame) -> DataFrame:
        """For each edge in the graph, work out its final position by joining
        on details of the destination node.

        Args:
            nodes: A polars dataframe containing details of all nodes in the
              graph
            edges: A polars dataframe containing details of all edges in the
              graph

        Returns:
            A copy of the edges dataframe with additional dst_lat, dst_lon,
            dst_easting and dst_northing columns

        """
        nodes = nodes.select(
            F.col("id").alias("dst"),
            F.col("lat").alias("dst_lat"),
            F.col("lon").alias("dst_lon"),
            F.col("easting").alias("dst_easting"),
            F.col("northing").alias("dst_northing"),
        )

        edges = edges.join(nodes, on="dst", how="inner")

        return edges

    # Ways
    # |-- id: long (nullable = true)
    # |-- version: integer (nullable = true)
    # |-- timestamp: long (nullable = true)
    # |-- changeset: long (nullable = true)
    # |-- uid: integer (nullable = true)
    # |-- user_sid: string (nullable = true)
    # |-- tags: array (nullable = true)
    # |    |-- element: struct (containsNull = true)
    # |    |    |-- key: string (nullable = true)
    # |    |    |-- value: string (nullable = true)
    # |-- nodes: array (nullable = true)
    # |    |-- element: struct (containsNull = true)
    # |    |    |-- index: integer (nullable = true)
    # |    |    |-- nodeId: long (nullable = true)

    @staticmethod
    def set_edge_output_schema(edges: DataFrame) -> DataFrame:
        """Ensure the edge output dataset contains only the required columns

        Args:
            edges: A polars dataframe containing details of all edges in the
              graph

        Returns:
            A subset of the provided dataframe

        """

        edges = edges.select(
            "src",
            "dst",
            "way_id",
            "way_inx",
            "highway",
            "surface",
            "is_flat",
            "src_lat",
            "src_lon",
            "dst_lat",
            "dst_lon",
            "src_easting",
            "src_northing",
            "dst_easting",
            "dst_northing",
        )
        return edges

    def write_to_parquet(self, df: DataFrame, target: str) -> None:
        tgt_loc = os.path.join(self.data_dir, "parsed", target)

        df.write.parquet(tgt_loc, mode="overwrite")

    def read_from_parquet(self, target: str) -> DataFrame:
        tgt_loc = os.path.join(self.data_dir, "parsed", target)

        df = self.spark.read.parquet(tgt_loc)

        return df

    def load(self) -> None:
        """End-to-end data load script for an OSM extract. This will read in
        the contents of the file using pyrosm, perform some basic processing to
        ensure the resultant graph will be bidirectional and way geometry
        will be preserved when processed by pyspark. Nodes and edges will be
        assigned to partitions according to eastings & northings before being
        written out to disk.
        """

        # nodes, ways = self.read_osm_data()

        # nodes = self.assign_bng_coords(nodes)
        # nodes = self.set_node_output_schema(nodes)

        # self.write_to_parquet(nodes, "nodes")
        nodes = self.read_from_parquet("nodes")

        # ways = self.unpack_tags(ways)
        # ways = self.remove_restricted_routes(ways)
        # ways = self.set_flat_flag(ways)
        # ways = self.set_oneway_flag(ways)
        # ways = self.get_tag_as_column(ways, "highway")
        # ways = self.get_tag_as_column(ways, "surface")

        # self.write_to_parquet(ways, "ways")
        ways = self.read_from_parquet("ways")

        edges = self.generate_edges(ways)
        edges = self.add_reverse_edges(edges)
        edges = self.get_edge_start_coords(nodes, edges)
        edges = self.get_edge_end_coords(nodes, edges)
        edges = self.set_edge_output_schema(edges)

        self.write_to_parquet(edges, "edges")
